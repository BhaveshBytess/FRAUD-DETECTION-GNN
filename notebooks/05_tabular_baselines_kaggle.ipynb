{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Baselines - Non-GNN Models\n",
    "\n",
    "**Goal:** Train traditional ML models on node features only (no graph structure)\n",
    "\n",
    "**Models:**\n",
    "1. Logistic Regression (Linear baseline)\n",
    "2. Random Forest (Tree ensemble)\n",
    "3. XGBoost (Gradient boosting)\n",
    "4. MLP (Neural network without graph)\n",
    "\n",
    "**Data:** Same 182 features, same temporal splits, **NO graph edges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data (Features Only - No Graph!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from GNN pipeline\n",
    "data_path = Path('/kaggle/input/elliptic-fraud-detection')\n",
    "\n",
    "# Load the PyG data object that was created\n",
    "import sys\n",
    "sys.path.append('/kaggle/working')\n",
    "\n",
    "# Actually, let's just load from the splits we already have\n",
    "# We'll use the same data loader but extract only features\n",
    "\n",
    "print(\"Loading Elliptic++ dataset...\")\n",
    "\n",
    "# Load raw CSVs\n",
    "features_df = pd.read_csv(data_path / 'txs_features.csv')\n",
    "classes_df = pd.read_csv(data_path / 'txs_classes.csv')\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Classes shape: {classes_df.shape}\")\n",
    "\n",
    "# Merge\n",
    "df = features_df.merge(classes_df, on='txId')\n",
    "print(f\"Merged shape: {df.shape}\")\n",
    "\n",
    "# Normalize timestamp column to 'timestamp'\n",
    "ts_candidates = ['Time step','time_step','timestamp','time','timestep']\n",
    "for c in ts_candidates:\n",
    "    if c in df.columns:\n",
    "        if c != 'timestamp':\n",
    "            df = df.rename(columns={c: 'timestamp'})\n",
    "        break\n",
    "else:\n",
    "    raise KeyError(f\"No timestamp column found in merged DataFrame; columns: {list(df.columns)[:25]}\")\n",
    "\n",
    "# Filter labeled only\n",
    "df_labeled = df[df['class'].isin(['1', '2'])].copy()\n",
    "df_labeled['label'] = (df_labeled['class'] == '2').astype(int)  # 2=fraud, 1=legit\n",
    "\n",
    "print(f\"Labeled transactions: {len(df_labeled)}\")\n",
    "print(f\"Fraud percentage: {df_labeled['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Temporal Splits (Same as GNN Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by timestamp\n",
    "df_labeled = df_labeled.sort_values('timestamp')\n",
    "\n",
    "# Split: 60% train, 20% val, 20% test (temporal)\n",
    "n = len(df_labeled)\n",
    "train_size = int(0.6 * n)\n",
    "val_size = int(0.2 * n)\n",
    "\n",
    "train_df = df_labeled.iloc[:train_size]\n",
    "val_df = df_labeled.iloc[train_size:train_size+val_size]\n",
    "test_df = df_labeled.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nTemporal Splits:\")\n",
    "print(f\"Train: {len(train_df)} ({len(train_df)/n*100:.1f}%) | Fraud: {train_df['label'].mean()*100:.2f}%\")\n",
    "print(f\"Val:   {len(val_df)} ({len(val_df)/n*100:.1f}%) | Fraud: {val_df['label'].mean()*100:.2f}%\")\n",
    "print(f\"Test:  {len(test_df)} ({len(test_df)/n*100:.1f}%) | Fraud: {test_df['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "feature_cols = [col for col in df_labeled.columns if col.startswith('LF') or col.startswith('AF')]\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"\\nNumber of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Sanitization (Same as GNN Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN and Inf (same preprocessing as GNN models)\n",
    "print(\"Sanitizing features...\")\n",
    "\n",
    "# Replace inf with NaN\n",
    "X_train = np.where(np.isinf(X_train), np.nan, X_train)\n",
    "X_val = np.where(np.isinf(X_val), np.nan, X_val)\n",
    "X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "\n",
    "# Fill NaN with 0\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Normalize (fit on train only)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Final NaN check\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "print(\"âœ… Features sanitized and normalized\")\n",
    "print(f\"Train NaN count: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Train Inf count: {np.isinf(X_train).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_probs, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model using same metrics as GNN models\n",
    "    \"\"\"\n",
    "    # PR-AUC (primary metric for imbalanced data)\n",
    "    pr_auc = average_precision_score(y_true, y_probs)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_probs)\n",
    "    \n",
    "    # Find optimal threshold using F1 score\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    # Recall at top-K%\n",
    "    def recall_at_k(y_true, y_probs, k_percent):\n",
    "        k = int(len(y_true) * k_percent / 100)\n",
    "        top_k_idx = np.argsort(y_probs)[-k:]\n",
    "        return y_true[top_k_idx].sum() / y_true.sum()\n",
    "    \n",
    "    recall_0_5 = recall_at_k(y_true, y_probs, 0.5)\n",
    "    recall_1_0 = recall_at_k(y_true, y_probs, 1.0)\n",
    "    recall_2_0 = recall_at_k(y_true, y_probs, 2.0)\n",
    "    \n",
    "    metrics = {\n",
    "        'pr_auc': float(pr_auc),\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'f1': float(best_f1),\n",
    "        'threshold': float(best_threshold),\n",
    "        'recall@0.5%': float(recall_0_5),\n",
    "        'recall@1.0%': float(recall_1_0),\n",
    "        'recall@2.0%': float(recall_2_0)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Test Set Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PR-AUC:       {pr_auc:.4f}\")\n",
    "    print(f\"ROC-AUC:      {roc_auc:.4f}\")\n",
    "    print(f\"F1 Score:     {best_f1:.4f} (threshold: {best_threshold:.4f})\")\n",
    "    print(f\"Recall@0.5%:  {recall_0_5:.4f}\")\n",
    "    print(f\"Recall@1.0%:  {recall_1_0:.4f}\")\n",
    "    print(f\"Recall@2.0%:  {recall_2_0:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weights (to handle imbalance)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# Train\n",
    "lr = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"âœ… Training complete!\")\n",
    "\n",
    "# Predict\n",
    "y_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics = evaluate_model(y_test, y_probs_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"âœ… Training complete!\")\n",
    "\n",
    "# Predict\n",
    "y_probs_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics = evaluate_model(y_test, y_probs_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 3: XGBoost (Expected Best Performer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training XGBoost...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    tree_method='hist',  # Fast\n",
    "    eval_metric='aucpr',\n",
    "    early_stopping_rounds=20,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with validation set for early stopping\n",
    "xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Best iteration: {xgb.best_iteration}\")\n",
    "\n",
    "# Predict\n",
    "y_probs_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics = evaluate_model(y_test, y_probs_xgb, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 4: MLP (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training MLP (Multi-Layer Perceptron)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,  # L2 regularization\n",
    "    batch_size=512,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(f\"\\nâœ… Training complete! Iterations: {mlp.n_iter_}\")\n",
    "\n",
    "# Predict\n",
    "y_probs_mlp = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "mlp_metrics = evaluate_model(y_test, y_probs_mlp, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN results from M3/M4\n",
    "gnn_results = {\n",
    "    'GCN': {\n",
    "        'pr_auc': 0.1976,\n",
    "        'roc_auc': 0.7627,\n",
    "        'f1': 0.2487,\n",
    "        'recall@1.0%': 0.0613\n",
    "    },\n",
    "    'GraphSAGE': {\n",
    "        'pr_auc': 0.4483,\n",
    "        'roc_auc': 0.8210,\n",
    "        'f1': 0.4527,\n",
    "        'recall@1.0%': 0.1478\n",
    "    },\n",
    "    'GAT': {\n",
    "        'pr_auc': 0.1839,\n",
    "        'roc_auc': 0.7942,\n",
    "        'f1': 0.2901,\n",
    "        'recall@1.0%': 0.0126\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add GNN models\n",
    "for model_name, metrics in gnn_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'GNN',\n",
    "        'PR-AUC': metrics['pr_auc'],\n",
    "        'ROC-AUC': metrics['roc_auc'],\n",
    "        'F1': metrics['f1'],\n",
    "        'Recall@1%': metrics['recall@1.0%']\n",
    "    })\n",
    "\n",
    "# Add ML models\n",
    "ml_results = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'XGBoost': xgb_metrics,\n",
    "    'MLP': mlp_metrics\n",
    "}\n",
    "\n",
    "for model_name, metrics in ml_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'ML',\n",
    "        'PR-AUC': metrics['pr_auc'],\n",
    "        'ROC-AUC': metrics['roc_auc'],\n",
    "        'F1': metrics['f1'],\n",
    "        'Recall@1%': metrics['recall@1.0%']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('PR-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON: ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model['Model']} ({best_model['Type']})\")\n",
    "print(f\"   PR-AUC: {best_model['PR-AUC']:.4f}\")\n",
    "print(f\"   ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   F1: {best_model['F1']:.4f}\")\n",
    "print(f\"   Recall@1%: {best_model['Recall@1%']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = ['PR-AUC', 'ROC-AUC', 'F1', 'Recall@1%']\n",
    "colors = ['#2ecc71' if t == 'GNN' else '#3498db' for t in comparison_df['Type']]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    bars = ax.barh(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, comparison_df[metric])):\n",
    "        ax.text(val + 0.01, i, f'{val:.4f}', va='center', fontsize=10)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='GNN (uses graph)'),\n",
    "    Patch(facecolor='#3498db', label='ML (features only)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization saved: all_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual model metrics\n",
    "for model_name, metrics in ml_results.items():\n",
    "    filename = f\"{model_name.lower().replace(' ', '_')}_metrics.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('all_models_comparison.csv', index=False)\n",
    "print(\"\\nâœ… Saved: all_models_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS TRAINED AND EVALUATED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights\n",
    "\n",
    "**Questions to Answer:**\n",
    "\n",
    "1. **Does the graph help?**\n",
    "   - If best GNN >> best ML â†’ Graph is essential\n",
    "   - If best ML â‰ˆ best GNN â†’ Features contain graph info already\n",
    "   - If best ML > best GNN â†’ Graph adds noise\n",
    "\n",
    "2. **Which model to deploy?**\n",
    "   - Best PR-AUC (primary metric for fraud)\n",
    "   - Best Recall@1% (catch most fraud in top predictions)\n",
    "   - Balance performance vs complexity\n",
    "\n",
    "3. **What did we learn?**\n",
    "   - Feature importance (from RF/XGBoost)\n",
    "   - Graph structure value (GNN vs ML gap)\n",
    "   - Temporal distribution shift handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
