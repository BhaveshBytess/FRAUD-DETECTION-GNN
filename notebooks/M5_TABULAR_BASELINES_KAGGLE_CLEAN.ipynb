{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5: Tabular ML Baselines - Does Graph Help?\n",
    "\n",
    "**Goal:** Train ML models WITHOUT graph structure and compare to GNN models.\n",
    "\n",
    "**Models:**\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- MLP (Neural Net)\n",
    "\n",
    "**Compare with:**\n",
    "- GraphSAGE (best GNN): PR-AUC = 0.4483\n",
    "\n",
    "**Dataset:** Elliptic++ (same 182 features, IGNORE graph edges)\n",
    "\n",
    "**Expected Runtime:** 15-20 minutes on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect dataset path\n",
    "kaggle_input = Path('/kaggle/input')\n",
    "dataset_folders = list(kaggle_input.glob('*'))\n",
    "\n",
    "print(f\"Available datasets: {[f.name for f in dataset_folders]}\")\n",
    "\n",
    "# Find dataset folder\n",
    "data_dir = None\n",
    "for folder in dataset_folders:\n",
    "    if (folder / 'elliptic_txs_features.csv').exists():\n",
    "        data_dir = folder\n",
    "        break\n",
    "\n",
    "if data_dir is None:\n",
    "    raise FileNotFoundError(\"‚ùå Dataset not found! Add 'elliptic-fraud-detection' in Kaggle data panel.\")\n",
    "\n",
    "print(f\"‚úÖ Using: {data_dir.name}\")\n",
    "\n",
    "# Load CSVs\n",
    "features_df = pd.read_csv(data_dir / 'elliptic_txs_features.csv', header=None)\n",
    "classes_df = pd.read_csv(data_dir / 'elliptic_txs_classes.csv')\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Classes shape: {classes_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "feature_cols = ['txId', 'Time step'] + [f'AF{i}' for i in range(1, 183)]\n",
    "features_df.columns = feature_cols\n",
    "\n",
    "# Merge features + classes\n",
    "df = features_df.merge(classes_df, on='txId', how='left')\n",
    "\n",
    "# Filter LABELED transactions only\n",
    "df_labeled = df[df['class'].isin(['1', '2'])].copy()\n",
    "df_labeled['label'] = df_labeled['class'].apply(lambda x: 1 if x == '2' else 0)\n",
    "\n",
    "print(f\"Total transactions: {len(df)}\")\n",
    "print(f\"Labeled transactions: {len(df_labeled)}\")\n",
    "print(f\"Fraud percentage: {df_labeled['label'].mean()*100:.2f}%\")\n",
    "print(f\"Time range: {df_labeled['Time step'].min()} to {df_labeled['Time step'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Temporal Splits (Same as GNN Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by timestamp\n",
    "df_labeled = df_labeled.sort_values('Time step').reset_index(drop=True)\n",
    "\n",
    "# Split: 60% train, 20% val, 20% test (temporal)\n",
    "n = len(df_labeled)\n",
    "train_size = int(0.6 * n)\n",
    "val_size = int(0.2 * n)\n",
    "\n",
    "train_df = df_labeled.iloc[:train_size]\n",
    "val_df = df_labeled.iloc[train_size:train_size+val_size]\n",
    "test_df = df_labeled.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nüìä Temporal Splits:\")\n",
    "print(f\"Train: {len(train_df):>6} ({len(train_df)/n*100:.1f}%) | Fraud: {train_df['label'].mean()*100:.2f}%\")\n",
    "print(f\"Val:   {len(val_df):>6} ({len(val_df)/n*100:.1f}%) | Fraud: {val_df['label'].mean()*100:.2f}%\")\n",
    "print(f\"Test:  {len(test_df):>6} ({len(test_df)/n*100:.1f}%) | Fraud: {test_df['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature columns (AF1-AF182)\n",
    "feature_names = [f'AF{i}' for i in range(1, 183)]\n",
    "\n",
    "X_train = train_df[feature_names].values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df[feature_names].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df[feature_names].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Standardize features (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features prepared\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_proba):\n",
    "    \"\"\"Calculate all metrics\"\"\"\n",
    "    pr_auc = average_precision_score(y_true, y_proba)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    # F1 with threshold 0.5\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Recall@1%\n",
    "    top_1pct_idx = np.argsort(y_proba)[-int(len(y_proba)*0.01):]\n",
    "    recall_at_1 = y_true[top_1pct_idx].mean()\n",
    "    \n",
    "    return {\n",
    "        'pr_auc': pr_auc,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_score': f1,\n",
    "        'recall_at_1pct': recall_at_1\n",
    "    }\n",
    "\n",
    "def print_results(model_name, metrics):\n",
    "    \"\"\"Pretty print results\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  PR-AUC:       {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  ROC-AUC:      {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  F1 Score:     {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Recall@1%:    {metrics['recall_at_1pct']:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Storage for all results\n",
    "all_results = {}\n",
    "\n",
    "print(\"‚úÖ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîµ Training Logistic Regression...\")\n",
    "\n",
    "# Calculate class weight\n",
    "fraud_ratio = y_train.mean()\n",
    "class_weight = {0: 1.0, 1: (1 - fraud_ratio) / fraud_ratio}\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=class_weight,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_proba = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "lr_metrics = evaluate_model(y_test, y_proba)\n",
    "all_results['Logistic Regression'] = lr_metrics\n",
    "print_results('Logistic Regression', lr_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest...\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rf_metrics = evaluate_model(y_test, y_proba)\n",
    "all_results['Random Forest'] = rf_metrics\n",
    "print_results('Random Forest', rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 3: XGBoost (Expected Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Training XGBoost...\")\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (1 - fraud_ratio) / fraud_ratio\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=SEED,\n",
    "    tree_method='hist',\n",
    "    eval_metric='aucpr',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_metrics = evaluate_model(y_test, y_proba)\n",
    "all_results['XGBoost'] = xgb_metrics\n",
    "print_results('XGBoost', xgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model 4: MLP (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Training MLP...\")\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_proba = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "mlp_metrics = evaluate_model(y_test, y_proba)\n",
    "all_results['MLP'] = mlp_metrics\n",
    "print_results('MLP', mlp_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare All Models (Including GNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GNN results for comparison\n",
    "gnn_results = {\n",
    "    'GCN': {'pr_auc': 0.1976, 'roc_auc': 0.7627, 'f1_score': 0.2487, 'recall_at_1pct': 0.0613},\n",
    "    'GAT': {'pr_auc': 0.1839, 'roc_auc': 0.7942, 'f1_score': 0.2901, 'recall_at_1pct': 0.0126},\n",
    "    'GraphSAGE': {'pr_auc': 0.4483, 'roc_auc': 0.8210, 'f1_score': 0.4527, 'recall_at_1pct': 0.1478}\n",
    "}\n",
    "\n",
    "all_results.update(gnn_results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "comparison_df['type'] = ['ML', 'ML', 'ML', 'ML', 'GNN', 'GNN', 'GNN']\n",
    "comparison_df = comparison_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  FINAL RESULTS - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find winners\n",
    "best_ml = comparison_df[comparison_df['type'] == 'ML'].iloc[0]\n",
    "best_gnn = comparison_df[comparison_df['type'] == 'GNN'].iloc[0]\n",
    "\n",
    "print(f\"\\nü•á Best ML Model:  {best_ml.name} (PR-AUC: {best_ml['pr_auc']:.4f})\")\n",
    "print(f\"ü•à Best GNN Model: {best_gnn.name} (PR-AUC: {best_gnn['pr_auc']:.4f})\")\n",
    "\n",
    "gap = (best_ml['pr_auc'] - best_gnn['pr_auc']) / best_gnn['pr_auc'] * 100\n",
    "print(f\"\\nüìä Gap: ML models are {gap:+.1f}% {'better' if gap > 0 else 'worse'} than GNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "metrics = ['pr_auc', 'roc_auc', 'f1_score', 'recall_at_1pct']\n",
    "titles = ['PR-AUC', 'ROC-AUC', 'F1 Score', 'Recall@1%']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    data = comparison_df.reset_index()\n",
    "    colors = ['blue' if t == 'ML' else 'green' for t in data['type']]\n",
    "    \n",
    "    ax.barh(data['index'], data[metric], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel(title, fontsize=12)\n",
    "    ax.set_title(f'{title} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add values\n",
    "    for i, v in enumerate(data[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: all_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual model metrics\n",
    "for model_name in ['Logistic Regression', 'Random Forest', 'XGBoost', 'MLP']:\n",
    "    if model_name in all_results:\n",
    "        filename = model_name.lower().replace(' ', '_') + '_metrics.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(all_results[model_name], f, indent=2)\n",
    "        print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "# Save comparison CSV\n",
    "comparison_df.to_csv('all_models_comparison.csv')\n",
    "print(f\"‚úÖ Saved: all_models_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  üì¶ FILES TO DOWNLOAD:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  1. logistic_regression_metrics.json\")\n",
    "print(\"  2. random_forest_metrics.json\")\n",
    "print(\"  3. xgboost_metrics.json\")\n",
    "print(\"  4. mlp_metrics.json\")\n",
    "print(\"  5. all_models_comparison.csv\")\n",
    "print(\"  6. all_models_comparison.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  üìä KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_ml['pr_auc'] > best_gnn['pr_auc']:\n",
    "    print(\"\\n‚úÖ RESULT: Tabular ML models OUTPERFORM GNNs!\")\n",
    "    print(f\"\\nBest ML (${best_ml.name}):   PR-AUC = {best_ml['pr_auc']:.4f}\")\n",
    "    print(f\"Best GNN (GraphSAGE): PR-AUC = {best_gnn['pr_auc']:.4f}\")\n",
    "    print(f\"\\nGap: {abs(gap):.1f}% better\")\n",
    "    \n",
    "    print(\"\\nüîç WHY?\")\n",
    "    print(\"  1. Dataset is 90%+ fraud (extreme imbalance)\")\n",
    "    print(\"  2. Features AF94-182 already contain neighbor aggregations\")\n",
    "    print(\"  3. GNNs propagate wrong labels from fraud-heavy neighborhoods\")\n",
    "    print(\"  4. Tabular models handle imbalance better with class weights\")\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATION:\")\n",
    "    print(f\"  ‚Üí Deploy {best_ml.name} for production\")\n",
    "    print(\"  ‚Üí Simpler, faster, no GPU needed\")\n",
    "    print(\"  ‚Üí Better performance than complex GNNs\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚úÖ RESULT: GNNs OUTPERFORM Tabular ML!\")\n",
    "    print(f\"\\nBest GNN (GraphSAGE): PR-AUC = {best_gnn['pr_auc']:.4f}\")\n",
    "    print(f\"Best ML ({best_ml.name}):  PR-AUC = {best_ml['pr_auc']:.4f}\")\n",
    "    print(f\"\\nGap: {abs(gap):.1f}% better\")\n",
    "    \n",
    "    print(\"\\nüîç WHY?\")\n",
    "    print(\"  ‚Üí Graph structure captures fraud patterns\")\n",
    "    print(\"  ‚Üí Message passing leverages network effects\")\n",
    "    print(\"  ‚Üí GNNs learn optimal neighbor aggregations\")\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATION:\")\n",
    "    print(\"  ‚Üí Deploy GraphSAGE for best performance\")\n",
    "    print(\"  ‚Üí Graph structure adds significant value\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  ‚úÖ M5 COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
