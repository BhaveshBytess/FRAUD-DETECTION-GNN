{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5: Tabular Baselines (No Graph Structure)\n",
    "\n",
    "**Question:** Does the graph help fraud detection, or are features alone sufficient?\n",
    "\n",
    "**Approach:**\n",
    "- Train ML models on node features ONLY (ignore edges)\n",
    "- Use SAME temporal splits as GNN models\n",
    "- Compare: Best ML vs GraphSAGE (0.4483 PR-AUC)\n",
    "\n",
    "**Models:**\n",
    "1. Logistic Regression (linear baseline)\n",
    "2. Random Forest (tree ensemble)\n",
    "3. XGBoost (gradient boosting - expected best)\n",
    "4. MLP (neural net, no graph)\n",
    "\n",
    "**Expected:** XGBoost PR-AUC ‚âà 0.25-0.35 (worse than GraphSAGE 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data & Temporal Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and labels\n",
    "data_dir = Path('../data/elliptic')\n",
    "\n",
    "features = pd.read_csv(data_dir / 'txs_features.csv')\n",
    "classes = pd.read_csv(data_dir / 'txs_classes.csv')\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Classes shape: {classes.shape}\")\n",
    "\n",
    "# Merge\n",
    "df = features.merge(classes, on='txId', how='left')\n",
    "\n",
    "# Rename timestamp column\n",
    "df = df.rename(columns={'Time step': 'timestamp'})\n",
    "\n",
    "print(f\"\\nMerged shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns[:5])}...\")\n",
    "\n",
    "# Filter to labeled only\n",
    "df_labeled = df[df['class'].isin([1, 2])].copy()\n",
    "df_labeled['label'] = (df_labeled['class'] == 2).astype(int)  # 2=fraud, 1=legit\n",
    "\n",
    "print(f\"\\nLabeled transactions: {len(df_labeled)}\")\n",
    "print(f\"Fraud percentage: {df_labeled['label'].mean()*100:.2f}%\")  # Should be ~8-10%\n",
    "print(f\"Timestamp range: {df_labeled['timestamp'].min()} to {df_labeled['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal splits (SAME AS GNN MODELS)\n",
    "# Train: timestamps 1-29\n",
    "# Val: timestamps 30-39  \n",
    "# Test: timestamps 40-49\n",
    "\n",
    "train_df = df_labeled[df_labeled['timestamp'] <= 29]\n",
    "val_df = df_labeled[(df_labeled['timestamp'] > 29) & (df_labeled['timestamp'] <= 39)]\n",
    "test_df = df_labeled[df_labeled['timestamp'] > 39]\n",
    "\n",
    "print(\"Temporal Splits:\")\n",
    "print(f\"Train: {len(train_df):5d} samples | Fraud: {train_df['label'].mean()*100:5.2f}% | Time: 1-29\")\n",
    "print(f\"Val:   {len(val_df):5d} samples | Fraud: {val_df['label'].mean()*100:5.2f}% | Time: 30-39\")\n",
    "print(f\"Test:  {len(test_df):5d} samples | Fraud: {test_df['label'].mean()*100:5.2f}% | Time: 40-49\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(set(train_df.index) & set(val_df.index)) == 0, \"Train-Val overlap!\"\n",
    "assert len(set(train_df.index) & set(test_df.index)) == 0, \"Train-Test overlap!\"\n",
    "assert len(set(val_df.index) & set(test_df.index)) == 0, \"Val-Test overlap!\"\n",
    "print(\"\\n‚úÖ No data leakage - splits are clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "feature_cols = [c for c in df_labeled.columns if c.startswith('Local_feature') or c.startswith('Aggregate_feature')]\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"Feature matrix shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_pr_auc = average_precision_score(y_test, lr_probs)\n",
    "lr_roc_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "# F1 with threshold from validation\n",
    "val_probs = lr_model.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[min(best_idx, len(thresholds)-1)]\n",
    "lr_preds = (lr_probs >= best_threshold).astype(int)\n",
    "lr_f1 = f1_score(y_test, lr_preds)\n",
    "\n",
    "# Recall@1%\n",
    "k = max(1, int(0.01 * len(y_test)))\n",
    "top_k_indices = np.argsort(lr_probs)[-k:]\n",
    "lr_recall_1pct = y_test[top_k_indices].sum() / y_test.sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Logistic Regression Results:\")\n",
    "print(f\"   PR-AUC:      {lr_pr_auc:.4f}\")\n",
    "print(f\"   ROC-AUC:     {lr_roc_auc:.4f}\")\n",
    "print(f\"   F1 Score:    {lr_f1:.4f}\")\n",
    "print(f\"   Recall@1%:   {lr_recall_1pct:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_pr_auc = average_precision_score(y_test, rf_probs)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_probs)\n",
    "\n",
    "# F1 with threshold from validation\n",
    "val_probs = rf_model.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[min(best_idx, len(thresholds)-1)]\n",
    "rf_preds = (rf_probs >= best_threshold).astype(int)\n",
    "rf_f1 = f1_score(y_test, rf_preds)\n",
    "\n",
    "# Recall@1%\n",
    "top_k_indices = np.argsort(rf_probs)[-k:]\n",
    "rf_recall_1pct = y_test[top_k_indices].sum() / y_test.sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Random Forest Results:\")\n",
    "print(f\"   PR-AUC:      {rf_pr_auc:.4f}\")\n",
    "print(f\"   ROC-AUC:     {rf_roc_auc:.4f}\")\n",
    "print(f\"   F1 Score:    {rf_f1:.4f}\")\n",
    "print(f\"   Recall@1%:   {rf_recall_1pct:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: XGBoost (Expected Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='aucpr'\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_pr_auc = average_precision_score(y_test, xgb_probs)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_probs)\n",
    "\n",
    "# F1 with threshold from validation\n",
    "val_probs = xgb_model.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[min(best_idx, len(thresholds)-1)]\n",
    "xgb_preds = (xgb_probs >= best_threshold).astype(int)\n",
    "xgb_f1 = f1_score(y_test, xgb_preds)\n",
    "\n",
    "# Recall@1%\n",
    "top_k_indices = np.argsort(xgb_probs)[-k:]\n",
    "xgb_recall_1pct = y_test[top_k_indices].sum() / y_test.sum()\n",
    "\n",
    "print(f\"\\n‚úÖ XGBoost Results:\")\n",
    "print(f\"   PR-AUC:      {xgb_pr_auc:.4f}\")\n",
    "print(f\"   ROC-AUC:     {xgb_roc_auc:.4f}\")\n",
    "print(f\"   F1 Score:    {xgb_f1:.4f}\")\n",
    "print(f\"   Recall@1%:   {xgb_recall_1pct:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: MLP (Neural Network, No Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MLP...\")\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "mlp_probs = mlp_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "mlp_pr_auc = average_precision_score(y_test, mlp_probs)\n",
    "mlp_roc_auc = roc_auc_score(y_test, mlp_probs)\n",
    "\n",
    "# F1 with threshold from validation\n",
    "val_probs = mlp_model.predict_proba(X_val)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[min(best_idx, len(thresholds)-1)]\n",
    "mlp_preds = (mlp_probs >= best_threshold).astype(int)\n",
    "mlp_f1 = f1_score(y_test, mlp_preds)\n",
    "\n",
    "# Recall@1%\n",
    "top_k_indices = np.argsort(mlp_probs)[-k:]\n",
    "mlp_recall_1pct = y_test[top_k_indices].sum() / y_test.sum()\n",
    "\n",
    "print(f\"\\n‚úÖ MLP Results:\")\n",
    "print(f\"   PR-AUC:      {mlp_pr_auc:.4f}\")\n",
    "print(f\"   ROC-AUC:     {mlp_roc_auc:.4f}\")\n",
    "print(f\"   F1 Score:    {mlp_f1:.4f}\")\n",
    "print(f\"   Recall@1%:   {mlp_recall_1pct:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare All Models (Tabular + GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN results (from M3/M4)\n",
    "gnn_results = {\n",
    "    'GCN': {'pr_auc': 0.1976, 'roc_auc': 0.7627, 'f1': 0.2487, 'recall_1pct': 0.0613},\n",
    "    'GraphSAGE': {'pr_auc': 0.4483, 'roc_auc': 0.8210, 'f1': 0.4527, 'recall_1pct': 0.1478},\n",
    "    'GAT': {'pr_auc': 0.1839, 'roc_auc': 0.7942, 'f1': 0.2901, 'recall_1pct': 0.0126}\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "results = {\n",
    "    'Model': [\n",
    "        'Logistic Regression', 'Random Forest', 'XGBoost', 'MLP',\n",
    "        'GCN', 'GraphSAGE', 'GAT'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'Tabular', 'Tabular', 'Tabular', 'Tabular',\n",
    "        'GNN', 'GNN', 'GNN'\n",
    "    ],\n",
    "    'PR-AUC': [\n",
    "        lr_pr_auc, rf_pr_auc, xgb_pr_auc, mlp_pr_auc,\n",
    "        gnn_results['GCN']['pr_auc'],\n",
    "        gnn_results['GraphSAGE']['pr_auc'],\n",
    "        gnn_results['GAT']['pr_auc']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        lr_roc_auc, rf_roc_auc, xgb_roc_auc, mlp_roc_auc,\n",
    "        gnn_results['GCN']['roc_auc'],\n",
    "        gnn_results['GraphSAGE']['roc_auc'],\n",
    "        gnn_results['GAT']['roc_auc']\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        lr_f1, rf_f1, xgb_f1, mlp_f1,\n",
    "        gnn_results['GCN']['f1'],\n",
    "        gnn_results['GraphSAGE']['f1'],\n",
    "        gnn_results['GAT']['f1']\n",
    "    ],\n",
    "    'Recall@1%': [\n",
    "        lr_recall_1pct, rf_recall_1pct, xgb_recall_1pct, mlp_recall_1pct,\n",
    "        gnn_results['GCN']['recall_1pct'],\n",
    "        gnn_results['GraphSAGE']['recall_1pct'],\n",
    "        gnn_results['GAT']['recall_1pct']\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('PR-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS: ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save\n",
    "df_results.to_csv('../reports/all_models_comparison.csv', index=False)\n",
    "print(\"\\n‚úÖ Saved: reports/all_models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# PR-AUC comparison\n",
    "ax = axes[0, 0]\n",
    "colors = ['blue' if t == 'Tabular' else 'green' for t in df_results['Type']]\n",
    "ax.barh(df_results['Model'], df_results['PR-AUC'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('PR-AUC', fontsize=12)\n",
    "ax.set_title('PR-AUC Comparison (Higher = Better)', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0.4483, color='red', linestyle='--', label='GraphSAGE (0.4483)')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ROC-AUC comparison\n",
    "ax = axes[0, 1]\n",
    "ax.barh(df_results['Model'], df_results['ROC-AUC'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('ROC-AUC', fontsize=12)\n",
    "ax.set_title('ROC-AUC Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "ax = axes[1, 0]\n",
    "ax.barh(df_results['Model'], df_results['F1 Score'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('F1 Score', fontsize=12)\n",
    "ax.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall@1% comparison\n",
    "ax = axes[1, 1]\n",
    "ax.barh(df_results['Model'], df_results['Recall@1%'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Recall@1%', fontsize=12)\n",
    "ax.set_title('Recall@1% (Fraud Detection Rate)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='blue', alpha=0.7, label='Tabular (No Graph)'),\n",
    "    Patch(facecolor='green', alpha=0.7, label='GNN (With Graph)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('../reports/plots/all_models_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Saved: reports/plots/all_models_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models\n",
    "best_tabular = df_results[df_results['Type'] == 'Tabular'].iloc[0]\n",
    "best_gnn = df_results[df_results['Type'] == 'GNN'].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Best Tabular Model: {best_tabular['Model']}\")\n",
    "print(f\"   PR-AUC: {best_tabular['PR-AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nüß† Best GNN Model: {best_gnn['Model']}\")\n",
    "print(f\"   PR-AUC: {best_gnn['PR-AUC']:.4f}\")\n",
    "\n",
    "gap = ((best_gnn['PR-AUC'] - best_tabular['PR-AUC']) / best_tabular['PR-AUC']) * 100\n",
    "\n",
    "print(f\"\\nüéØ Graph Structure Impact:\")\n",
    "print(f\"   PR-AUC Improvement: {gap:+.1f}%\")\n",
    "\n",
    "if gap > 20:\n",
    "    conclusion = \"üèÜ Graph is ESSENTIAL! GNNs win decisively.\"\n",
    "    recommendation = \"Deploy GraphSAGE for best fraud detection.\"\n",
    "elif gap > 5:\n",
    "    conclusion = \"‚úÖ Graph helps moderately. GNNs worth the complexity.\"\n",
    "    recommendation = \"GraphSAGE recommended, but XGBoost viable for simpler deployment.\"\n",
    "elif gap > -5:\n",
    "    conclusion = \"‚öñÔ∏è  Roughly equal. Choose based on operational requirements.\"\n",
    "    recommendation = \"XGBoost for simplicity, GraphSAGE for marginal gain.\"\n",
    "else:\n",
    "    conclusion = \"‚ö†Ô∏è  Tabular wins! Graph adds noise.\"\n",
    "    recommendation = \"Deploy XGBoost. GNNs not justified.\"\n",
    "\n",
    "print(f\"\\n{conclusion}\")\n",
    "print(f\"\\nüí° Recommendation: {recommendation}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Individual Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual results\n",
    "models_data = [\n",
    "    ('logistic_regression', lr_pr_auc, lr_roc_auc, lr_f1, lr_recall_1pct),\n",
    "    ('random_forest', rf_pr_auc, rf_roc_auc, rf_f1, rf_recall_1pct),\n",
    "    ('xgboost', xgb_pr_auc, xgb_roc_auc, xgb_f1, xgb_recall_1pct),\n",
    "    ('mlp', mlp_pr_auc, mlp_roc_auc, mlp_f1, mlp_recall_1pct)\n",
    "]\n",
    "\n",
    "for model_name, pr_auc, roc_auc, f1, recall_1pct in models_data:\n",
    "    metrics = {\n",
    "        'test_pr_auc': float(pr_auc),\n",
    "        'test_roc_auc': float(roc_auc),\n",
    "        'test_f1': float(f1),\n",
    "        'test_recall_1pct': float(recall_1pct)\n",
    "    }\n",
    "    \n",
    "    with open(f'../reports/{model_name}_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"‚úÖ Saved: reports/{model_name}_metrics.json\")\n",
    "\n",
    "print(\"\\n‚úÖ M5 COMPLETE! All artifacts saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
