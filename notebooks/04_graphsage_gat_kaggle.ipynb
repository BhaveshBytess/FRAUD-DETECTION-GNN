{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAGE & GAT - Kaggle GPU Training\n",
    "\n",
    "Training GraphSAGE and GAT models on Elliptic++ dataset.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU enabled (Settings → Accelerator → GPU T4 x2)\n",
    "- Elliptic dataset linked (Add Data → elliptic-fraud-detection)\n",
    "\n",
    "**Expected Runtime:** ~25-30 minutes (both models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-geometric -q\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Seed set to 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset (Same as GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/kaggle/input/elliptic-fraud-detection/'\n",
    "\n",
    "print(\"Loading Elliptic++ dataset...\")\n",
    "\n",
    "features_df = pd.read_csv(DATA_PATH + 'txs_features.csv')\n",
    "classes_df = pd.read_csv(DATA_PATH + 'txs_classes.csv')\n",
    "edges_df = pd.read_csv(DATA_PATH + 'txs_edgelist.csv')\n",
    "\n",
    "print(f\"✓ Features: {features_df.shape}\")\n",
    "print(f\"✓ Classes: {classes_df.shape}\")\n",
    "print(f\"✓ Edges: {edges_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and process\n",
    "data_df = features_df.merge(classes_df, on='txId', how='left')\n",
    "data_df['class'] = data_df['class'].fillna(3).astype(int)\n",
    "\n",
    "print(f\"Total: {len(data_df):,}\")\n",
    "print(f\"Fraud: {(data_df['class'] == 1).sum():,}\")\n",
    "print(f\"Legit: {(data_df['class'] == 2).sum():,}\")\n",
    "print(f\"Unlabeled: {(data_df['class'] == 3).sum():,}\")\n",
    "\n",
    "# Features\n",
    "feature_cols = [col for col in data_df.columns if col not in ['txId', 'Time step', 'class']]\n",
    "feat_df = data_df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "x_np = feat_df.astype(np.float32).values\n",
    "\n",
    "# Normalize\n",
    "mean = x_np.mean(axis=0)\n",
    "std = x_np.std(axis=0)\n",
    "std[std < 1e-6] = 1.0\n",
    "x_np = (x_np - mean) / std\n",
    "\n",
    "x = torch.from_numpy(x_np).float()\n",
    "x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Labels\n",
    "timestamps = data_df['Time step'].values\n",
    "y_raw = data_df['class'].values\n",
    "y = np.where(y_raw == 1, 1, np.where(y_raw == 2, 0, -1))\n",
    "y = torch.LongTensor(y)\n",
    "\n",
    "print(f\"\\n✓ Features: {x.shape}\")\n",
    "print(f\"✓ Labels: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "tx_ids = data_df['txId'].values\n",
    "tx_id_to_idx = {tx_id: idx for idx, tx_id in enumerate(tx_ids)}\n",
    "\n",
    "valid_edges = edges_df[\n",
    "    edges_df['txId1'].isin(tx_id_to_idx) & \n",
    "    edges_df['txId2'].isin(tx_id_to_idx)\n",
    "]\n",
    "\n",
    "edge_src = valid_edges['txId1'].map(tx_id_to_idx).values\n",
    "edge_dst = valid_edges['txId2'].map(tx_id_to_idx).values\n",
    "edge_index = torch.LongTensor(np.vstack([edge_src, edge_dst]))\n",
    "\n",
    "# Add self-loops\n",
    "num_nodes = len(data_df)\n",
    "self_loop_src = np.arange(num_nodes)\n",
    "self_loop_dst = np.arange(num_nodes)\n",
    "edge_index = torch.cat([\n",
    "    edge_index,\n",
    "    torch.LongTensor(np.vstack([self_loop_src, self_loop_dst]))\n",
    "], dim=1)\n",
    "\n",
    "print(f\"✓ Edges (with self-loops): {edge_index.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal splits\n",
    "sorted_times = np.sort(np.unique(timestamps))\n",
    "n_timesteps = len(sorted_times)\n",
    "\n",
    "train_end_idx = int(n_timesteps * 0.6)\n",
    "val_end_idx = int(n_timesteps * 0.8)\n",
    "\n",
    "train_time_end = sorted_times[train_end_idx - 1]\n",
    "val_time_end = sorted_times[val_end_idx - 1]\n",
    "\n",
    "labeled_mask = y >= 0\n",
    "train_mask = torch.BoolTensor((timestamps <= train_time_end) & labeled_mask.numpy())\n",
    "val_mask = torch.BoolTensor((timestamps > train_time_end) & (timestamps <= val_time_end) & labeled_mask.numpy())\n",
    "test_mask = torch.BoolTensor((timestamps > val_time_end) & labeled_mask.numpy())\n",
    "\n",
    "print(f\"Train: {train_mask.sum():,}\")\n",
    "print(f\"Val:   {val_mask.sum():,}\")\n",
    "print(f\"Test:  {test_mask.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSAGE Model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128, out_channels=2, num_layers=2, dropout=0.4):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        if num_layers > 1:\n",
    "            self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        else:\n",
    "            self.convs[0] = SAGEConv(in_channels, out_channels)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "# GAT Model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128, out_channels=2, num_layers=2, heads=4, dropout=0.4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout, concat=True))\n",
    "        \n",
    "        input_dim = hidden_channels * heads\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(input_dim, hidden_channels, heads=heads, dropout=dropout, concat=True))\n",
    "        \n",
    "        if num_layers > 1:\n",
    "            self.convs.append(GATConv(input_dim, out_channels, heads=heads, dropout=dropout, concat=False))\n",
    "        else:\n",
    "            self.convs[0] = GATConv(in_channels, out_channels, heads=heads, dropout=dropout, concat=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "print(\"✓ Models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move data to device\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "val_mask = val_mask.to(device)\n",
    "test_mask = test_mask.to(device)\n",
    "\n",
    "print(f\"✓ Data on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GraphSAGE\n",
    "model_sage = GraphSAGE(in_channels=x.shape[1], hidden_channels=128, out_channels=2, num_layers=2, dropout=0.4)\n",
    "model_sage = model_sage.to(device)\n",
    "\n",
    "print(f\"✓ GraphSAGE params: {sum(p.numel() for p in model_sage.parameters()):,}\")\n",
    "\n",
    "optimizer_sage = torch.optim.Adam(model_sage.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nTraining GraphSAGE...\")\n",
    "\n",
    "best_val_pr_auc = 0\n",
    "best_epoch = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "history_sage = {'train_loss': [], 'val_loss': [], 'val_pr_auc': []}\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train\n",
    "    model_sage.train()\n",
    "    optimizer_sage.zero_grad()\n",
    "    out = model_sage(x, edge_index)\n",
    "    \n",
    "    if torch.isnan(out).any():\n",
    "        print(f\"Epoch {epoch+1}: NaN, skipping\")\n",
    "        continue\n",
    "    \n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_sage.parameters(), 1.0)\n",
    "    optimizer_sage.step()\n",
    "    \n",
    "    # Validate\n",
    "    model_sage.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_sage(x, edge_index)\n",
    "        if torch.isnan(out).any():\n",
    "            continue\n",
    "        \n",
    "        val_loss = criterion(out[val_mask], y[val_mask]).item()\n",
    "        val_probs = F.softmax(out[val_mask], dim=1)[:, 1].cpu().numpy()\n",
    "        val_labels = y[val_mask].cpu().numpy()\n",
    "        \n",
    "        if np.isnan(val_probs).any():\n",
    "            continue\n",
    "        \n",
    "        val_pr_auc = average_precision_score(val_labels, val_probs)\n",
    "    \n",
    "    history_sage['train_loss'].append(loss.item())\n",
    "    history_sage['val_loss'].append(val_loss)\n",
    "    history_sage['val_pr_auc'].append(val_pr_auc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}: Train={loss.item():.4f}, Val={val_loss:.4f}, PR-AUC={val_pr_auc:.4f}\")\n",
    "    \n",
    "    if val_pr_auc > best_val_pr_auc:\n",
    "        best_val_pr_auc = val_pr_auc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        best_state_sage = model_sage.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "if best_val_pr_auc > 0:\n",
    "    model_sage.load_state_dict(best_state_sage)\n",
    "    print(f\"\\n✓ GraphSAGE complete! Best Val PR-AUC: {best_val_pr_auc:.4f} at epoch {best_epoch+1}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No valid training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GAT\n",
    "model_gat = GAT(in_channels=x.shape[1], hidden_channels=64, out_channels=2, num_layers=2, heads=4, dropout=0.4)\n",
    "model_gat = model_gat.to(device)\n",
    "\n",
    "print(f\"✓ GAT params: {sum(p.numel() for p in model_gat.parameters()):,}\")\n",
    "\n",
    "optimizer_gat = torch.optim.Adam(model_gat.parameters(), lr=0.005, weight_decay=0.0005)\n",
    "\n",
    "print(\"\\nTraining GAT...\")\n",
    "\n",
    "best_val_pr_auc_gat = 0\n",
    "best_epoch_gat = 0\n",
    "patience_counter_gat = 0\n",
    "history_gat = {'train_loss': [], 'val_loss': [], 'val_pr_auc': []}\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train\n",
    "    model_gat.train()\n",
    "    optimizer_gat.zero_grad()\n",
    "    out = model_gat(x, edge_index)\n",
    "    \n",
    "    if torch.isnan(out).any():\n",
    "        print(f\"Epoch {epoch+1}: NaN, skipping\")\n",
    "        continue\n",
    "    \n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_gat.parameters(), 1.0)\n",
    "    optimizer_gat.step()\n",
    "    \n",
    "    # Validate\n",
    "    model_gat.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_gat(x, edge_index)\n",
    "        if torch.isnan(out).any():\n",
    "            continue\n",
    "        \n",
    "        val_loss = criterion(out[val_mask], y[val_mask]).item()\n",
    "        val_probs = F.softmax(out[val_mask], dim=1)[:, 1].cpu().numpy()\n",
    "        val_labels = y[val_mask].cpu().numpy()\n",
    "        \n",
    "        if np.isnan(val_probs).any():\n",
    "            continue\n",
    "        \n",
    "        val_pr_auc = average_precision_score(val_labels, val_probs)\n",
    "    \n",
    "    history_gat['train_loss'].append(loss.item())\n",
    "    history_gat['val_loss'].append(val_loss)\n",
    "    history_gat['val_pr_auc'].append(val_pr_auc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}: Train={loss.item():.4f}, Val={val_loss:.4f}, PR-AUC={val_pr_auc:.4f}\")\n",
    "    \n",
    "    if val_pr_auc > best_val_pr_auc_gat:\n",
    "        best_val_pr_auc_gat = val_pr_auc\n",
    "        best_epoch_gat = epoch\n",
    "        patience_counter_gat = 0\n",
    "        best_state_gat = model_gat.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_gat += 1\n",
    "    \n",
    "    if patience_counter_gat >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "if best_val_pr_auc_gat > 0:\n",
    "    model_gat.load_state_dict(best_state_gat)\n",
    "    print(f\"\\n✓ GAT complete! Best Val PR-AUC: {best_val_pr_auc_gat:.4f} at epoch {best_epoch_gat+1}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No valid training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        test_probs = F.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n",
    "        test_labels = y[test_mask].cpu().numpy()\n",
    "        \n",
    "        val_probs = F.softmax(out[val_mask], dim=1)[:, 1].cpu().numpy()\n",
    "        val_labels = y[val_mask].cpu().numpy()\n",
    "    \n",
    "    # Metrics\n",
    "    test_pr_auc = average_precision_score(test_labels, test_probs)\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_probs)\n",
    "    \n",
    "    # Threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(val_labels, val_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n",
    "    \n",
    "    test_preds = (test_probs >= best_threshold).astype(int)\n",
    "    test_f1 = f1_score(test_labels, test_preds)\n",
    "    \n",
    "    # Recall@K\n",
    "    def recall_at_k(y_true, y_score, k_frac):\n",
    "        k = max(1, int(len(y_true) * k_frac))\n",
    "        top_k_idx = np.argsort(y_score)[-k:]\n",
    "        return y_true[top_k_idx].sum() / y_true.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name} TEST RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PR-AUC:      {test_pr_auc:.4f}\")\n",
    "    print(f\"ROC-AUC:     {test_roc_auc:.4f}\")\n",
    "    print(f\"F1 Score:    {test_f1:.4f}\")\n",
    "    print(f\"Threshold:   {best_threshold:.4f}\")\n",
    "    print(f\"Recall@0.5%: {recall_at_k(test_labels, test_probs, 0.005):.4f}\")\n",
    "    print(f\"Recall@1.0%: {recall_at_k(test_labels, test_probs, 0.01):.4f}\")\n",
    "    print(f\"Recall@2.0%: {recall_at_k(test_labels, test_probs, 0.02):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'pr_auc': float(test_pr_auc),\n",
    "        'roc_auc': float(test_roc_auc),\n",
    "        'f1': float(test_f1),\n",
    "        'threshold': float(best_threshold),\n",
    "        'recall@0.5%': float(recall_at_k(test_labels, test_probs, 0.005)),\n",
    "        'recall@1.0%': float(recall_at_k(test_labels, test_probs, 0.01)),\n",
    "        'recall@2.0%': float(recall_at_k(test_labels, test_probs, 0.02))\n",
    "    }\n",
    "\n",
    "metrics_sage = evaluate_model(model_sage, \"GraphSAGE\")\n",
    "metrics_gat = evaluate_model(model_gat, \"GAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "with open('graphsage_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_sage, f, indent=2)\n",
    "\n",
    "with open('gat_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_gat, f, indent=2)\n",
    "\n",
    "# Save models\n",
    "torch.save(model_sage.state_dict(), 'graphsage_best.pt')\n",
    "torch.save(model_gat.state_dict(), 'gat_best.pt')\n",
    "\n",
    "print(\"\\n✓ All results saved!\")\n",
    "print(\"\\nDownload:\")\n",
    "print(\"  - graphsage_metrics.json\")\n",
    "print(\"  - gat_metrics.json\")\n",
    "print(\"  - graphsage_best.pt\")\n",
    "print(\"  - gat_best.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
