{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Baseline for Fraud Detection\n",
    "\n",
    "This notebook trains a Graph Convolutional Network (GCN) on the Elliptic++ dataset for Bitcoin transaction fraud detection.\n",
    "\n",
    "**Goal:** Establish a reproducible GCN baseline with temporal splits and honest evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook TODO\n",
    "- [x] Load real Elliptic++ from `data/elliptic/`\n",
    "- [x] Set seeds + deterministic flags\n",
    "- [x] Train GCN model end-to-end\n",
    "- [x] Save: `reports/metrics.json`, `reports/plots/*.png`, append `reports/metrics_summary.csv`\n",
    "- [x] Verify metrics + artifacts paths printed in last cell\n",
    "- [x] Clear TODOs/placeholders before commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data import EllipticDataset\n",
    "from src.models.gcn import GCN, GCNTrainer\n",
    "from src.utils.seed import set_all_seeds\n",
    "from src.utils.metrics import (\n",
    "    compute_metrics,\n",
    "    find_best_f1_threshold,\n",
    "    compute_recall_at_k\n",
    ")\n",
    "from src.utils.logger import save_metrics_json, append_metrics_to_csv\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_all_seeds(SEED)\n",
    "print(f\"‚úÖ All seeds set to {SEED}\")\n",
    "print(\"‚úÖ Deterministic operations enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Elliptic++ dataset\n",
    "dataset = EllipticDataset(root='../data/elliptic')\n",
    "data = dataset.load(verbose=True)\n",
    "\n",
    "print(\"\\nüìä Dataset Summary:\")\n",
    "print(f\"   Nodes: {data.x.shape[0]:,}\")\n",
    "print(f\"   Edges: {data.edge_index.shape[1]:,}\")\n",
    "print(f\"   Features: {data.x.shape[1]}\")\n",
    "print(f\"   Train nodes: {data.train_mask.sum():,}\")\n",
    "print(f\"   Val nodes: {data.val_mask.sum():,}\")\n",
    "print(f\"   Test nodes: {data.test_mask.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'in_channels': data.x.shape[1],\n",
    "    'hidden_channels': 128,\n",
    "    'out_channels': 2,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.4\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = GCN(**config)\n",
    "print(f\"\\n‚úÖ GCN Model initialized\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Architecture: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = GCNTrainer(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    device=device,\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "# Train with early stopping\n",
    "history = trainer.fit(\n",
    "    epochs=100,\n",
    "    patience=15,\n",
    "    eval_metric='pr_auc',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Best validation PR-AUC: {trainer.best_val_metric:.4f}\")\n",
    "print(f\"   Best epoch: {trainer.best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].axvline(trainer.best_epoch, color='r', linestyle='--', alpha=0.5, label='Best Epoch')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metric plot\n",
    "axes[1].plot(history['val_metric'], label='Val PR-AUC', linewidth=2, color='green')\n",
    "axes[1].axvline(trainer.best_epoch, color='r', linestyle='--', alpha=0.5, label='Best Epoch')\n",
    "axes[1].axhline(trainer.best_val_metric, color='g', linestyle=':', alpha=0.5, label=f'Best: {trainer.best_val_metric:.4f}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('PR-AUC')\n",
    "axes[1].set_title('Validation PR-AUC Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/plots/gcn_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Get test predictions\n",
    "test_loss, test_preds, test_probs = trainer.evaluate(data.test_mask)\n",
    "test_labels = data.y[data.test_mask].cpu().numpy()\n",
    "test_probs_fraud = test_probs[:, 1].cpu().numpy()\n",
    "\n",
    "print(\"üìä Test Set Evaluation:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Find best threshold on validation set\n",
    "val_loss, val_preds, val_probs = trainer.evaluate(data.val_mask)\n",
    "val_labels = data.y[data.val_mask].cpu().numpy()\n",
    "val_probs_fraud = val_probs[:, 1].cpu().numpy()\n",
    "\n",
    "best_threshold, best_f1_val = find_best_f1_threshold(val_labels, val_probs_fraud)\n",
    "print(f\"\\n   Best threshold (from val): {best_threshold:.4f}\")\n",
    "print(f\"   Val F1 at best threshold: {best_f1_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute comprehensive metrics on test set\n",
    "test_metrics = compute_metrics(test_labels, test_probs_fraud, threshold=best_threshold)\n",
    "recall_at_k = compute_recall_at_k(test_labels, test_probs_fraud, k_fracs=[0.005, 0.01, 0.02])\n",
    "\n",
    "# Combine metrics\n",
    "test_metrics.update(recall_at_k)\n",
    "\n",
    "print(\"\\nüìà Test Set Metrics:\")\n",
    "print(f\"   PR-AUC:      {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"   ROC-AUC:     {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"   F1 Score:    {test_metrics['f1']:.4f}\")\n",
    "print(f\"   Threshold:   {test_metrics['threshold']:.4f}\")\n",
    "print(f\"\\n   Recall@0.5%: {test_metrics['recall@0.5%']:.4f}\")\n",
    "print(f\"   Recall@1.0%: {test_metrics['recall@1.0%']:.4f}\")\n",
    "print(f\"   Recall@2.0%: {test_metrics['recall@2.0%']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot PR and ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute curves\n",
    "precision, recall, pr_thresholds = precision_recall_curve(test_labels, test_probs_fraud)\n",
    "fpr, tpr, roc_thresholds = roc_curve(test_labels, test_probs_fraud)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PR Curve\n",
    "axes[0].plot(recall, precision, linewidth=2.5, label=f'GCN (PR-AUC={test_metrics[\"pr_auc\"]:.4f})')\n",
    "axes[0].axhline(test_labels.mean(), color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Baseline (random)')\n",
    "axes[0].set_xlabel('Recall', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curve (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC Curve\n",
    "axes[1].plot(fpr, tpr, linewidth=2.5, label=f'GCN (ROC-AUC={test_metrics[\"roc_auc\"]:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Baseline (random)')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title('ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/plots/gcn_pr_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ PR and ROC curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint_path = Path('../checkpoints/gcn_best.pt')\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'metrics': test_metrics,\n",
    "    'best_epoch': trainer.best_epoch,\n",
    "    'seed': SEED\n",
    "}, checkpoint_path)\n",
    "print(f\"‚úÖ Model checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "# Save metrics JSON\n",
    "metrics_json_path = Path('../reports/gcn_metrics.json')\n",
    "save_metrics_json(test_metrics, metrics_json_path)\n",
    "print(f\"‚úÖ Metrics JSON saved: {metrics_json_path}\")\n",
    "\n",
    "# Append to summary CSV\n",
    "summary_csv_path = Path('../reports/metrics_summary.csv')\n",
    "append_metrics_to_csv(\n",
    "    metrics=test_metrics,\n",
    "    filepath=summary_csv_path,\n",
    "    experiment_name='elliptic-gnn-baselines',\n",
    "    model_name='GCN',\n",
    "    split='test'\n",
    ")\n",
    "print(f\"‚úÖ Results appended to: {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GCN BASELINE - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Total nodes: {data.x.shape[0]:,}\")\n",
    "print(f\"   Total edges: {data.edge_index.shape[1]:,}\")\n",
    "print(f\"   Features: {data.x.shape[1]}\")\n",
    "print(f\"\\nüéØ Model: GCN\")\n",
    "print(f\"   Hidden channels: {config['hidden_channels']}\")\n",
    "print(f\"   Num layers: {config['num_layers']}\")\n",
    "print(f\"   Dropout: {config['dropout']}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nüìà Test Results:\")\n",
    "print(f\"   PR-AUC:      {test_metrics['pr_auc']:.4f} ‚≠ê (primary metric)\")\n",
    "print(f\"   ROC-AUC:     {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"   F1 Score:    {test_metrics['f1']:.4f}\")\n",
    "print(f\"   Recall@0.5%: {test_metrics['recall@0.5%']:.4f}\")\n",
    "print(f\"   Recall@1.0%: {test_metrics['recall@1.0%']:.4f}\")\n",
    "print(f\"   Recall@2.0%: {test_metrics['recall@2.0%']:.4f}\")\n",
    "print(f\"\\nüìÅ Artifacts Saved:\")\n",
    "print(f\"   ‚úÖ {checkpoint_path}\")\n",
    "print(f\"   ‚úÖ {metrics_json_path}\")\n",
    "print(f\"   ‚úÖ {summary_csv_path}\")\n",
    "print(f\"   ‚úÖ ../reports/plots/gcn_training_history.png\")\n",
    "print(f\"   ‚úÖ ../reports/plots/gcn_pr_roc_curves.png\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GCN BASELINE COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
