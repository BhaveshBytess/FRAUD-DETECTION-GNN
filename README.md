# ğŸ” When Graph Neural Networks Fail
## Revisiting Graph Learning on the Elliptic++ Bitcoin Fraud Detection Dataset

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

### ğŸ¯ **TL;DR**

**Graph Neural Networks (GNNs) are supposed to excel at graph-structured data. But on Elliptic++ Bitcoin fraud detection, a simple XGBoost model beats all GNN baselines by 49%.**

This repository investigates **why** â€” and reveals that pre-computed neighbor aggregates in the features make GNNs redundant.

---

### ğŸ”¬ **The Surprising Finding**

> **Main Result:** XGBoost (tabular-only, no graph) achieves **PR-AUC 0.669**, while GraphSAGE (state-of-the-art GNN) achieves only **0.448**.
> 
> **Why?** Features `AF94â€“AF182` already encode neighbor-aggregated information. Removing them:
> - âœ… GraphSAGE **improves by 24%** (0.448 â†’ 0.556)
> - âœ… XGBoost **drops only 3%** (0.669 â†’ 0.649)
> 
> **Conclusion:** Graph structure *does* add value â€” but only when features don't already capture it.

---

### ğŸ“Š **Performance Comparison**

We trained 7 models using strict temporal splits (no leakage) on the Elliptic++ dataset:

| Model Type | Model | PR-AUC â­ | ROC-AUC | F1 | Recall@1% |
|------------|-------|--------:|--------:|----:|----------:|
| ğŸŒ³ **Tabular** | **XGBoost** | **0.669** ğŸ¥‡ | 0.888 | 0.699 | 17.5% |
| ğŸŒ³ Tabular | Random Forest | 0.658 ğŸ¥ˆ | 0.877 | 0.694 | 17.5% |
| ğŸ•¸ï¸ **GNN** | **GraphSAGE** | **0.448** ğŸ¥‰ | 0.821 | 0.453 | 14.8% |
| ğŸŒ Tabular | MLP | 0.364 | 0.830 | 0.486 | 9.4% |
| ğŸ•¸ï¸ GNN | GCN | 0.198 | 0.763 | 0.249 | 6.1% |
| ğŸ•¸ï¸ GNN | GAT | 0.184 | 0.794 | 0.290 | 1.3% |

<div align="center">

![Model Performance Comparison](reports/plots/all_models_comparison.png)

**Figure 1:** XGBoost (tabular) significantly outperforms all GNN baselines on fraud detection.

</div>

> ğŸ“Œ **Key Insight:** The **49% performance gap** (0.669 vs 0.448) between XGBoost and GraphSAGE led us to investigate feature dominance â€” see ablation results in [`docs/M7_RESULTS.md`](docs/M7_RESULTS.md).

---

## ğŸš€ **Quick Start**

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (optional, for GNN training)
- ~2GB disk space for dataset

### Installation & Reproduction

```bash
# 1ï¸âƒ£ Clone and setup environment
git clone https://github.com/BhaveshBytess/FRAUD-DETECTION-GNN.git
cd FRAUD-DETECTION-GNN
python -m venv venv && source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2ï¸âƒ£ Download Elliptic++ dataset (NOT included in repo)
# Get from: https://drive.google.com/drive/folders/1MRPXz79Lu_JGLlJ21MDfML44dKN9R08l
# Place these files in: data/Elliptic++ Dataset/
#   â”œâ”€â”€ txs_features.csv
#   â”œâ”€â”€ txs_classes.csv
#   â””â”€â”€ txs_edgelist.csv

# 3ï¸âƒ£ Verify data loading
python -m src.data.elliptic_loader --root "data/Elliptic++ Dataset" --check

# 4ï¸âƒ£ Reproduce results
# Train GNN baseline (GPU recommended, ~15 min)
python -m src.train --config configs/graphsage.yaml

# Train tabular baselines (CPU, ~2 min)
python scripts/run_m5_tabular.py --config configs/m5_xgboost.yaml

# 5ï¸âƒ£ View results
ls reports/  # Metrics JSON/CSV files
ls reports/plots/  # Figures
```

**Expected Output:** Metrics files in `reports/` matching our published results (Â±2% variance due to randomness).

---

## ğŸ“¦ **Dataset**

### Elliptic++ Bitcoin Transaction Network

| Property | Value |
|----------|-------|
| **Nodes** | 203,769 Bitcoin transactions |
| **Edges** | 234,355 transaction flows |
| **Features** | 182 per transaction (local + aggregated) |
| **Labels** | Licit (89%) / Illicit (11%) |
| **Timespan** | 49 timesteps (temporal graph) |
| **Task** | Binary fraud classification |

âš ï¸ **Dataset NOT included** â€” Download from [Google Drive](https://drive.google.com/drive/folders/1MRPXz79Lu_JGLlJ21MDfML44dKN9R08l) (public access, no sign-in required)

**Required files:**
```
data/Elliptic++ Dataset/
â”œâ”€â”€ txs_features.csv       (203K rows Ã— 182 features)
â”œâ”€â”€ txs_classes.csv        (node labels)
â””â”€â”€ txs_edgelist.csv       (graph edges)
```

**Citation for dataset:**
> Weber, M., et al. (2019). "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics." *KDD Workshop on Anomaly Detection in Finance*.

---

## ğŸ“š **Project Structure & Documentation**

```
FRAUD-DETECTION-GNN/
â”œâ”€â”€ ğŸ“„ README.md                    â† You are here (landing page)
â”œâ”€â”€ ğŸ“˜ docs/
â”‚   â”œâ”€â”€ README_FULL.md              â† Complete technical documentation
â”‚   â”œâ”€â”€ PROJECT_SPEC.md             â† Architecture & acceptance criteria
â”‚   â”œâ”€â”€ M5_RESULTS_SUMMARY.md       â† Tabular baseline results
â”‚   â”œâ”€â”€ M7_RESULTS.md               â† ğŸ”¬ Feature ablation experiments
â”‚   â”œâ”€â”€ M8_INTERPRETABILITY.md      â† SHAP & GNN saliency analysis
â”‚   â””â”€â”€ M9_TEMPORAL.md              â† Temporal robustness study
â”œâ”€â”€ ğŸ“Š reports/
â”‚   â”œâ”€â”€ metrics_summary.csv         â† All model results
â”‚   â””â”€â”€ plots/                      â† Figures (PNG)
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”œâ”€â”€ 03_gcn_baseline.ipynb       â† GNN training workflows
â”‚   â”œâ”€â”€ 05_m5_tabular.ipynb         â† XGBoost/RF experiments
â”‚   â”œâ”€â”€ 06_m7_ablation.ipynb        â† Feature ablation analysis
â”‚   â”œâ”€â”€ 07_interpretability.ipynb   â† SHAP & saliency
â”‚   â””â”€â”€ 08_temporal_shift.ipynb     â† Temporal generalization
â”œâ”€â”€ ğŸ§  src/                         â† Modular source code
â”‚   â”œâ”€â”€ data/elliptic_loader.py     â† Dataset loader with splits
â”‚   â”œâ”€â”€ models/                     â† GNN & tabular model definitions
â”‚   â”œâ”€â”€ train.py                    â† Training script
â”‚   â””â”€â”€ eval.py                     â† Evaluation pipeline
â”œâ”€â”€ âš™ï¸ configs/                     â† YAML configs per model
â””â”€â”€ ğŸ’¾ checkpoints/                 â† Trained model weights
```

### ğŸ”— **Key Documents**

| Document | Description |
|----------|-------------|
| ğŸ“˜ [**Full Documentation**](docs/README_FULL.md) | Complete technical README (~10 min read) |
| ğŸ“„ [**Project Report**](PROJECT_REPORT.md) | Analysis, discussion, and findings |
| ğŸ”¬ [**Feature Ablation**](docs/M7_RESULTS.md) | Why AF94â€“AF182 explain the GNN gap |
| ğŸ§  [**Interpretability**](docs/M8_INTERPRETABILITY.md) | SHAP (XGBoost) + saliency (GraphSAGE) |
| â±ï¸ [**Temporal Study**](docs/M9_TEMPORAL.md) | Generalization across time windows |

---

## ğŸ† **Why This Project Matters**

### Research Contributions
1. **Empirical rigor:** Strict temporal splits, no leakage, reproducible baselines
2. **Unexpected finding:** Tabular models outperform GNNs by 49% on graph data
3. **Root cause analysis:** Feature ablation reveals double-encoding of graph structure
4. **Practical insight:** Graph features can make graph models redundant

### Use Cases
- ğŸ“ **Students/Researchers:** Reproducible baseline for GNN vs ML comparisons
- ğŸ’¼ **Data Scientists:** When to use (or avoid) GNNs in fraud detection
- ğŸ¦ **Financial ML Teams:** Feature engineering insights for transaction networks
- ğŸ“š **Educators:** Teaching case study on ablation studies & interpretability

---

## ğŸ“– **Citation**

If you use this code or findings, please cite:

```bibtex
@software{elliptic_gnn_baselines_2025,
  title = {When Graph Neural Networks Fail: Revisiting Graph Learning on the Elliptic++ Dataset},
  author = {Bytes, Bhavesh},
  year = {2025},
  url = {https://github.com/BhaveshBytess/FRAUD-DETECTION-GNN},
  license = {MIT}
}
```

**Machine-readable citation:** See [`CITATION.cff`](CITATION.cff)

---

## ğŸ“¬ **Contact & License**

**Author:** Bhavesh Bytes  
**Email:** 10bhavesh7.11@gmail.com  
**GitHub:** [@BhaveshBytess](https://github.com/BhaveshBytess)  
**License:** [MIT License](LICENSE)

**Project Status:** âœ… Complete (M1â€“M9) | **Last Updated:** November 2025

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star!**

[![GitHub stars](https://img.shields.io/github/stars/BhaveshBytess/FRAUD-DETECTION-GNN?style=social)](https://github.com/BhaveshBytess/FRAUD-DETECTION-GNN/stargazers)

</div>
