{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47db1c90",
   "metadata": {},
   "source": [
    "# M5 â€” Tabular Baselines (Local)\n",
    "\n",
    "**Goal:** Train strong tabular models (LR, RF, XGBoost, MLP) on the corrected Elliptic++ labels to answer *\"Does graph structure help?\"*\n",
    "\n",
    "## Notebook TODO (auto-discipline)\n",
    "- [ ] Load real Elliptic++ dataset from `data/Elliptic++ Dataset/`\n",
    "- [ ] Set deterministic seeds via `src.utils.seed`\n",
    "- [ ] Train LR/RF/XGBoost/MLP end-to-end (no graph features)\n",
    "- [ ] Save metrics JSON + comparison CSV + plots under `reports/`\n",
    "- [ ] Print artifact paths + best metrics in the final cell\n",
    "- [ ] Clear this TODO checklist before marking M5 done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455594f8",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.utils.seed import set_all_seeds\n",
    "from scripts import run_m5_tabular as m5\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"Elliptic++ Dataset\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "PLOTS_DIR = REPORTS_DIR / \"plots\"\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing Elliptic++ dataset at {DATA_DIR}\")\n",
    "\n",
    "set_all_seeds(42)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "print(f\"Project root : {PROJECT_ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Reports dir  : {REPORTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3a812",
   "metadata": {},
   "source": [
    "## 2. Load Elliptic++ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b815b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data = m5.load_tabular_dataset(DATA_DIR)\n",
    "\n",
    "stats = pd.Series(tab_data.stats).to_frame(name='value')\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbf426",
   "metadata": {},
   "source": [
    "### Peek at training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data.train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe94d5e",
   "metadata": {},
   "source": [
    "## 3. Train tabular models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result = m5.train_logistic_regression(tab_data)\n",
    "pd.Series(lr_result.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result = m5.train_random_forest(tab_data)\n",
    "pd.Series(rf_result.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result = m5.train_xgboost(tab_data)\n",
    "pd.Series(xgb_result.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_result = m5.train_mlp(tab_data)\n",
    "pd.Series(mlp_result.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a7e18",
   "metadata": {},
   "source": [
    "## 4. Aggregate + compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a785cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [lr_result, rf_result, xgb_result, mlp_result]\n",
    "summary_df = m5.summarize_results(results)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0593d",
   "metadata": {},
   "source": [
    "## 5. Persist artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.save_artifacts(results, tab_data.y_test, REPORTS_DIR)\n",
    "print(\"Saved:\")\n",
    "for path in [REPORTS_DIR / 'logistic_regression_metrics.json',\n",
    "             REPORTS_DIR / 'random_forest_metrics.json',\n",
    "             REPORTS_DIR / 'xgboost_metrics.json',\n",
    "             REPORTS_DIR / 'mlp_metrics.json',\n",
    "             REPORTS_DIR / 'all_models_comparison.csv',\n",
    "             REPORTS_DIR / 'plots' / 'all_models_comparison.png']:\n",
    "    print(f\"  - {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220b4a1",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = summary_df.iloc[0]\n",
    "print(f\"Best tabular model : {best_row['model']}\")\n",
    "print(f\"Test PR-AUC        : {best_row['pr_auc']:.4f}\")\n",
    "print(f\"Test ROC-AUC       : {best_row['roc_auc']:.4f}\")\n",
    "print(f\"Test F1 (val thr)  : {best_row['f1']:.4f}\")\n",
    "print(f\"Recall@1%          : {best_row['recall@1.0%']:.4f}\")\n",
    "print(\"\n",
    "Next step: compare against retrained GraphSAGE/GCN/GAT baselines once available.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
