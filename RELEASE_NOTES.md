# v1.0.0: Elliptic++ Fraud Detection - When Do GNNs Add Value?

## ğŸ¯ Release Summary

This release marks the completion of a comprehensive comparative study investigating **when Graph Neural Networks provide marginal value over tabular ML models** for fraud detection on the Elliptic++ Bitcoin transaction dataset.

**Main Finding:** Features already encode graph aggregations â€” GNNs redundant unless features are raw.

---

## ğŸ”¬ Key Research Contributions

### 1. Feature Dominance Hypothesis â€” CONFIRMED âœ…

Through ablation experiments (M7), we confirmed that tabular features AF94â€“AF182 already encode neighbor-aggregated information:

| Model | Config | PR-AUC | Î” vs Full | Interpretation |
|-------|--------|--------|-----------|----------------|
| **XGBoost** | Full (AF1â€“182) | 0.669 | â€” | Baseline |
| XGBoost | Local only (AF1â€“93) | 0.648 | **âˆ’3%** | **Barely affected** |
| **GraphSAGE** | Full (AF1â€“182) | 0.448 | â€” | Redundant encoding |
| GraphSAGE | Local only (AF1â€“93) | **0.556** | **+24%** | **GNN unlocked!** |

**Evidence:**
- Neighbor averages correlate **r=0.74â€“0.89** with aggregate features
- Manual graph metrics correlate **r=0.63â€“0.65** with aggregates
- AF94â€“AF182 are literally pre-computed neighbor aggregations

### 2. Interpretability Analysis (M8)

- **XGBoost (full):** Heavily relies on aggregate features (SHAP analysis)
- **GraphSAGE (local-only):** Learns from graph structure via message passing
- Models learn *different* representations: pre-computed features vs dynamic aggregation

### 3. Temporal Robustness (M9)

- **XGBoost:** Stable 0.67â€“0.78 PR-AUC across time shifts
- **GraphSAGE (local):** Improves 0.41 â†’ 0.56 with earlier training windows (+35%)
- **Finding:** GNNs handle temporal drift better when trained on raw features

---

## ğŸ“Š Complete Results

**Model Performance Rankings (PR-AUC):**

| Rank | Model | Type | PR-AUC | ROC-AUC | F1 | Recall@1% |
|------|-------|------|--------|---------|----|-----------| 
| ğŸ¥‡ 1 | **XGBoost** | Tabular | **0.669** | 0.888 | 0.699 | 0.175 |
| ğŸ¥ˆ 2 | Random Forest | Tabular | 0.658 | 0.877 | 0.694 | 0.175 |
| ğŸ¥‰ 3 | **GraphSAGE** | GNN | **0.448** | 0.821 | 0.453 | 0.148 |
| 4 | MLP | Tabular | 0.364 | 0.830 | 0.486 | 0.094 |
| 5 | GCN | GNN | 0.198 | 0.763 | 0.249 | 0.061 |
| 6 | GAT | GNN | 0.184 | 0.794 | 0.290 | 0.013 |
| 7 | Logistic Regression | Tabular | 0.164 | 0.824 | 0.256 | 0.005 |

---

## ğŸ“ Milestones Completed (10/10)

- âœ… **M1:** Repository scaffold & infrastructure
- âœ… **M2:** Dataset loader with temporal splits
- âœ… **M3:** GCN baseline implementation
- âœ… **M4:** GraphSAGE & GAT models
- âœ… **M5:** Tabular baselines (LR, RF, XGBoost, MLP)
- âœ… **M6:** Documentation & comparative analysis
- âœ… **M7:** Causality & Feature Dominance â€” **HYPOTHESIS CONFIRMED**
- âœ… **M8:** Interpretability (SHAP + GNN saliency)
- âœ… **M9:** Temporal Robustness Study
- âœ… **M10:** Final polish & release preparation

---

## ğŸ“ What's Included

### Documentation
- **`PROJECT_REPORT.md`** â€” Publication-style comprehensive report (13KB)
- **`README.md`** â€” Complete project overview with findings
- **`PROJECT_SUMMARY.md`** â€” Detailed narrative and evidence
- **`docs/M7_RESULTS.md`** â€” Feature dominance ablation results
- **`docs/M8_INTERPRETABILITY.md`** â€” SHAP + GNN saliency analysis
- **`docs/M9_TEMPORAL.md`** â€” Temporal robustness findings
- **`LICENSE`** â€” MIT License

### Code & Notebooks
- 8 reproducible notebooks (`notebooks/03-08_*.ipynb`)
- 12+ training scripts (`scripts/run_m*.py`)
- Model implementations (GCN, GraphSAGE, GAT)
- Complete data pipeline

### Artifacts
- **25+ result files** in `reports/`
- M7 ablation experiments (7 CSV/JSON files)
- M8 interpretability (3 CSV/JSON + plots)
- M9 temporal robustness (1 CSV file)
- All baseline model metrics

---

## ğŸ’¡ Key Takeaways

> **Graph structure is valuable â€” but dataset features already captured it through pre-computed aggregations.**

**For Practitioners:**
- âœ… Use **XGBoost** when features include graph aggregations (fast, interpretable, CPU-friendly)
- âœ… Use **GNNs** when features are raw and graph structure is critical
- âœ… Always check feature-structure redundancy before selecting models

**For Researchers:**
- Feature engineering quality matters more than model architecture
- Ablation studies reveal hidden feature-structure redundancies
- Correlation analysis essential for understanding feature provenance
- Temporal robustness tests reveal model generalization characteristics

---

## ğŸš€ Getting Started

```bash
# Clone repository
git clone https://github.com/BhaveshBytess/FRAUD-DETECTION-GNN.git
cd FRAUD-DETECTION-GNN

# Install dependencies
pip install -r requirements.txt

# Explore results
jupyter notebook notebooks/

# Read full report
cat PROJECT_REPORT.md
```

---

## ğŸ“– Citation

If you use this work, please cite:

```bibtex
@misc{elliptic-gnn-2025,
  title={Elliptic++ Fraud Detection: When Do Graph Neural Networks Add Value?},
  author={Bhavesh Bytes},
  year={2025},
  url={https://github.com/BhaveshBytess/FRAUD-DETECTION-GNN},
  note={v1.0.0}
}
```

---

## ğŸ‰ Acknowledgments

- Elliptic++ dataset providers
- PyTorch Geometric community
- Kaggle for GPU resources

---

**Project Status:** âœ… Complete  
**Release Date:** 2025-11-08  
**License:** MIT

---

**â­ If you find this project useful, please star the repository!**
